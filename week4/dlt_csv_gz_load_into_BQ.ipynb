{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Prepare dlt"
      ],
      "metadata": {
        "id": "8W4mfgDiTz6X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9_JDID1xTTyE"
      },
      "outputs": [],
      "source": [
        " !pip install dlt requests\n",
        " #!pip install \"dlt[bigquery]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Authenticate with Google Cloud\n",
        "from google.colab import files\n",
        "import os"
      ],
      "metadata": {
        "id": "x-spvjSjUJ8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your service account key file\n",
        "uploaded = files.upload()\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "SscMgDADXy_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Write the code to load data from a URL pattern into BigQuery\n",
        "import dlt\n",
        "import gzip\n",
        "import csv\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Define a function to read compressed CSV.GZ files from a public URL\n",
        "def read_compressed_csv_from_url(url):\n",
        "    # Fetch the file from the public URL\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code != 200:\n",
        "        raise ValueError(f\"Failed to fetch file from URL: {url}\")\n",
        "\n",
        "    # Decompress the file and read it as CSV\n",
        "    with gzip.GzipFile(fileobj=BytesIO(response.content), mode='rb') as file:\n",
        "        reader = csv.DictReader(file.read().decode('utf-8').splitlines())\n",
        "        for row in reader:\n",
        "            yield row\n",
        "\n",
        "# Define a pipeline to load data into BigQuery\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"taxi_tripdata_to_bigquery\",\n",
        "    destination=\"bigquery\",\n",
        "    dataset_name=\"trips_data_all\",  # Replace with your BigQuery dataset name\n",
        ")\n",
        "\n",
        "# Define the base URL template\n",
        "base_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/{}/{}_tripdata_{}-{}.csv.gz\"\n",
        "\n",
        "# Define the taxi types to load (green, yellow, or fhv)\n",
        "taxi_types = [\"yellow\", \"fhv\",\"green\"]  # Add more types if needed\n",
        "\n",
        "# Define the range of years and months to load\n",
        "start_year = 2019\n",
        "end_year = 2020\n",
        "start_month = 1\n",
        "end_month = 12\n",
        "\n",
        "# Load data from each URL into BigQuery\n",
        "for taxi_type in taxi_types:\n",
        "    # Adjust the year range for fhv data (only 2019)\n",
        "    if taxi_type == \"fhv\":\n",
        "        years_to_load = [2019]\n",
        "    else:\n",
        "        years_to_load = range(start_year, end_year + 1)\n",
        "\n",
        "    for year in years_to_load:\n",
        "        for month in range(start_month, end_month + 1):\n",
        "            # Format the month to two digits (e.g., 01, 02, ..., 12)\n",
        "            month_str = f\"{month:02d}\"\n",
        "            url = base_url.format(taxi_type, taxi_type, year, month_str)  # Generate the URL using the template\n",
        "\n",
        "            try:\n",
        "                load_info = pipeline.run(\n",
        "                    read_compressed_csv_from_url(url),\n",
        "                    table_name=f\"{taxi_type}_tripdata\",  # Use taxi type in the table name\n",
        "                    write_disposition=\"append\",  # Use \"append\" to add data from multiple files\n",
        "                )\n",
        "                print(f\"Loaded data from {url}: {load_info}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load data from {url}: {e}\")"
      ],
      "metadata": {
        "id": "8YObE0rOdmZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comments\n",
        "- If the taxi_type is \"fhv\", the years_to_load variable is set to [2019] (only 2019).\n",
        "- For other taxi types (green and yellow), the years_to_load variable is set to range(start_year, end_year + 1) (2019 and 2020).\n",
        "\n",
        "Dynamic URL Generation:\n",
        "The base_url template remains the same, but the year loop is adjusted based on the taxi_type.\n",
        "\n",
        "## Table Name:\n",
        "The table_name in the pipeline.run method is dynamically set based on the taxi type:\n",
        "\n",
        "## Python\n",
        "Copy\n",
        "table_name=f\"{taxi_type}_tripdata\"\n",
        "This ensures that green, yellow, and fhv trip data are loaded into separate tables (e.g., green_tripdata, yellow_tripdata, and fhv_tripdata).\n",
        "\n",
        "## Error Handling:\n",
        "If a URL fails to load (e.g., the file does not exist), the error is caught and logged without stopping the pipeline.\n",
        "\n",
        "## Notes:\n",
        "Dynamic URL Generation:\n",
        "You can modify the URL template and range of years/months to match your file naming convention.\n",
        "Add more robust error handling if needed (e.g., retry logic for failed URLs).\n",
        "\n",
        "## Performance:\n",
        "For large numbers of files, consider using parallel processing or batching to optimize performance.\n",
        "\n",
        "## Schema Consistency:\n",
        "Ensure all .csv.gz files for each taxi type have the same schema (column names and data types). If not, you may need to handle schema mismatches.\n",
        "This approach allows you to dynamically generate and load green, yellow, and fhv taxi trip data into separate BigQuery tables, with the condition that fhv data is only loaded for the year 2019."
      ],
      "metadata": {
        "id": "5JQi8QjIyhlB"
      }
    }
  ]
}